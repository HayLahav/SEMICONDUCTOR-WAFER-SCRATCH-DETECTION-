# ğŸ”¬ Semiconductor Wafer Scratch Detection - Enhanced Hybrid Model
## Advanced Deep Learning Solution for Automated Defect Detection in Semiconductor Manufacturing
### ğŸ“‹ Table of Contents

Overview
Problem Statement
Solution Architecture
Key Innovations
Results
Installation & Setup
Usage
Model Architecture
Visualization
Future Work

## ğŸ¯ Overview
This project addresses a critical challenge in semiconductor manufacturing: automated detection of linear scratch patterns and "ink dies" (good dies within scratches) on silicon wafers. The solution combines multiple state-of-the-art deep learning models to achieve sophisticated pattern recognition with high accuracy.
Business Impact

Cost Reduction: Automate expensive manual inspection processes
Speed: Real-time detection vs hours of manual work
Accuracy: Consistent detection criteria across all wafers
Yield Optimization: Better balance between quality and productivity

## ğŸ” Problem Statement
In semiconductor manufacturing, wafers contain hundreds of individual chips (dies). Scratches appear as elongated clusters of defective dies and significantly impact production quality. Key challenges include:

Discontinuous Patterns: Scratches often have gaps
Ink Die Detection: Good dies within scratches are hard to detect
Diagonal Scratches: More challenging than horizontal/vertical patterns
Noise vs Real Defects: Single bad dies might be noise, not scratches
Limited Training Data: Inconsistent labeling in datasets

## ğŸ—ï¸ Solution Architecture
Enhanced Hybrid Model Components

UNet Backbone: Semantic segmentation for initial scratch detection
Perception Encoder (PE): Global context understanding
SmolVLM Integration: Vision-language model for pattern reasoning
Multi-Task Learning: Separate heads for scratch and ink detection
Adaptive Processing: Context-aware thresholding and post-processing

Input (64x64) â†’ UNet + PE + SmolVLM â†’ Feature Fusion â†’ Multi-Task Heads â†’ Adaptive Threshold â†’ Post-Processing
ğŸ’¡ Key Innovations
ğŸ¯ Targeted Loss Function

5x weight penalty for ink dies
2x weight for scratch dies
Focal loss for handling class imbalance

## ğŸ”„ Adaptive Thresholding

Standard threshold (0.5) for bad dies
Lower threshold (0.3) for potential ink regions
Spatial context awareness for density-based adjustments

## ğŸ§  Multi-Modal Learning

UNet: Local pattern detection
PE: Global wafer context
SmolVLM: Semantic understanding of scratch patterns

## ğŸ”§ Context-Aware Post-Processing

RANSAC line fitting for gap bridging
DBSCAN clustering for component analysis
Morphological operations for pattern refinement

## ğŸ“Š Results
Performance Metrics

Overall Recall: 81% on linear scratch detection
Ink Detection: Specialized detection with adaptive thresholding

Model Capabilities

âœ… Linear scratch pattern detection
âœ… Ink die identification within scratches
âœ… Gap bridging in discontinuous patterns
âœ… Multi-orientation scratch detection
âœ… Real-time inference capability

## ğŸš€ Installation & Setup
Requirements

Google Colab with L4 GPU (recommended)
Python 3.8+
PyTorch 2.0+
CUDA-compatible GPU

### Dependencies
bashpip install transformers accelerate pillow scikit-learn scipy
pip install diffusers torch torchvision
pip install hf_xet decord ftfy
Model Components
bash# Clone perception models repository
git clone https://github.com/facebookresearch/perception_models.git

### ğŸ’» Usage

1. Data Preparation
python# Load wafer data
df_wafers = pd.read_csv('wafers_train.csv')
df_wafers_test = pd.read_csv('wafers_test.csv')

Create dataset

dataset = WaferMapDataset(df_wafers, wafer_size=64, yield_threshold=0.4)
2. Model Training
python# Initialize components
unet_model = setup_model(device)
enhanced_model = SmolVLMEnhancedHybridModel(unet_model, use_smolvlm=True)

# Train model
trained_model = train_enhanced_hybrid_model(
    unet_model, train_loader, val_loader, device,
    epochs=10, lr=0.5e-4
)
3. Inference
python# Make predictions
predictions_df = predict_with_enhanced_model(
    model, test_loader, device, df_test,
    use_adaptive_threshold=True,
    use_post_processing=True
)
4. Visualization
python# Visualize results
visualize_enhanced_predictions_with_ground_truth(
    model, dataset, device, num_samples=2
)
### ğŸ›ï¸ Model Architecture
Network Structure
Input Wafer Map (64x64x1)
â”œâ”€â”€ UNet Backbone (Frozen)
â”œâ”€â”€ Perception Encoder (Global Context)
â”œâ”€â”€ SmolVLM (Pattern Understanding)
â””â”€â”€ Feature Fusion
    â”œâ”€â”€ Residual Blocks
    â”œâ”€â”€ Spatial Attention
    â””â”€â”€ Multi-Task Heads
        â”œâ”€â”€ Scratch Detection Head
        â””â”€â”€ Ink Detection Head
Key Components
UNet Backbone

Pre-trained diffusion model
Local feature extraction
Semantic segmentation capabilities

Perception Encoder

Global context understanding
Wafer-level pattern recognition
Feature normalization

SmolVLM Integration

Vision-language understanding
Semantic scratch pattern analysis
Pattern reasoning capabilities

## ğŸ“ˆ Visualization
The model provides comprehensive visualization including:
ğŸ–¼ï¸ Multi-Panel Displays

Input wafer maps (good/bad dies)
Ground truth labels
Model predictions with color coding
Probability heatmaps
Individual head outputs
Adaptive threshold maps

ğŸ¨ Color Coding

Green: Correctly detected scratches
Red: Detected ink dies
Blue: Binary predictions
Heat maps: Probability distributions

## ğŸ”® Future Work
Current Issues

 dtype Mismatch: Fix float16/float32 compatibility for full SmolVLM integration
 Memory Optimization: Further reduce GPU memory requirements
 Speed Optimization: Improve inference time for production deployment

Planned Enhancements

 Advanced Augmentation: Synthetic scratch generation
 Transformer Integration: Add directional attention mechanisms
 Multi-Scale Detection: Handle various scratch sizes
 Edge Deployment: Optimize for production environments
 Active Learning: Incorporate human feedback loop

ğŸ™ Acknowledgments

UNet2DModel: From HuggingFace Diffusers
Perception Encoder: Facebook Research perception_models
SmolVLM: HuggingFace Vision-Language Model
Google Colab: L4 GPU infrastructure

